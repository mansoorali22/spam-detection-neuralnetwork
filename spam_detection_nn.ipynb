{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":213216,"sourceType":"datasetVersion","datasetId":91827},{"sourceId":209156580,"sourceType":"kernelVersion"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Word2Vec Embeddings as Input","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pickle\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:34.598566Z","iopub.execute_input":"2024-11-23T11:51:34.598944Z","iopub.status.idle":"2024-11-23T11:51:34.604168Z","shell.execute_reply.started":"2024-11-23T11:51:34.598913Z","shell.execute_reply":"2024-11-23T11:51:34.602836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_path = '/kaggle/input/nlp-a3-word2vec'\n\n# Load word embeddings\nwith open(os.path.join(data_path, 'word_embeddings.pkl'), 'rb') as f:\n    word_embeddings = pickle.load(f)\n\n# Load vocabulary\nwith open(os.path.join(data_path, 'vocabulary.pkl'), 'rb') as f:\n    vocabulary = pickle.load(f)\n\n# Load word2idx mapping\nwith open(os.path.join(data_path, 'word2idx.pkl'), 'rb') as f:\n    word2idx = pickle.load(f)\n\n# Load idx2word mapping\nwith open(os.path.join(data_path, 'idx2word.pkl'), 'rb') as f:\n    idx2word = pickle.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:34.614273Z","iopub.execute_input":"2024-11-23T11:51:34.614700Z","iopub.status.idle":"2024-11-23T11:51:34.656298Z","shell.execute_reply.started":"2024-11-23T11:51:34.614664Z","shell.execute_reply":"2024-11-23T11:51:34.655234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Loaded {len(word_embeddings)} word embeddings.\")\nprint(f\"Vocabulary size: {len(vocabulary)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:34.658529Z","iopub.execute_input":"2024-11-23T11:51:34.659327Z","iopub.status.idle":"2024-11-23T11:51:34.665136Z","shell.execute_reply.started":"2024-11-23T11:51:34.659272Z","shell.execute_reply":"2024-11-23T11:51:34.663938Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Email Dataset","metadata":{}},{"cell_type":"markdown","source":"## Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:34.666350Z","iopub.execute_input":"2024-11-23T11:51:34.666782Z","iopub.status.idle":"2024-11-23T11:51:34.680095Z","shell.execute_reply.started":"2024-11-23T11:51:34.666732Z","shell.execute_reply":"2024-11-23T11:51:34.678745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/spam-or-not-spam-dataset/spam_or_not_spam.csv')\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:34.682366Z","iopub.execute_input":"2024-11-23T11:51:34.682835Z","iopub.status.idle":"2024-11-23T11:51:34.760349Z","shell.execute_reply.started":"2024-11-23T11:51:34.682788Z","shell.execute_reply":"2024-11-23T11:51:34.759070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['label'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:34.762176Z","iopub.execute_input":"2024-11-23T11:51:34.762524Z","iopub.status.idle":"2024-11-23T11:51:34.771355Z","shell.execute_reply.started":"2024-11-23T11:51:34.762488Z","shell.execute_reply":"2024-11-23T11:51:34.770117Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### Remove missing values","metadata":{}},{"cell_type":"code","source":"# Identify the row with the missing email\nmissing_email_index = df[df['email'].isnull()].index\n\n# Drop the row\ndf = df.drop(missing_email_index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:34.772755Z","iopub.execute_input":"2024-11-23T11:51:34.773102Z","iopub.status.idle":"2024-11-23T11:51:34.786455Z","shell.execute_reply.started":"2024-11-23T11:51:34.773052Z","shell.execute_reply":"2024-11-23T11:51:34.785409Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Balancing Dataset","metadata":{}},{"cell_type":"code","source":"# Separate spam and not spam messages\nspam_df = df[df['label'] == 1]\nnot_spam_df = df[df['label'] == 0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:34.789302Z","iopub.execute_input":"2024-11-23T11:51:34.789778Z","iopub.status.idle":"2024-11-23T11:51:34.800212Z","shell.execute_reply.started":"2024-11-23T11:51:34.789714Z","shell.execute_reply":"2024-11-23T11:51:34.798885Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Undersampling Not Spam messages","metadata":{}},{"cell_type":"code","source":"# Randomly sample 500 not spam messages\nnot_spam_sampled_df = not_spam_df.sample(n=500, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:34.801543Z","iopub.execute_input":"2024-11-23T11:51:34.801942Z","iopub.status.idle":"2024-11-23T11:51:34.812105Z","shell.execute_reply.started":"2024-11-23T11:51:34.801879Z","shell.execute_reply":"2024-11-23T11:51:34.810997Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Combine balanced dataset","metadata":{}},{"cell_type":"code","source":"# Concatenate the spam messages with the sampled not spam messages\nbalanced_df = pd.concat([spam_df, not_spam_sampled_df])\n\n# Shuffle the dataset to mix spam and not spam messages\nbalanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:34.813811Z","iopub.execute_input":"2024-11-23T11:51:34.814295Z","iopub.status.idle":"2024-11-23T11:51:34.825291Z","shell.execute_reply.started":"2024-11-23T11:51:34.814246Z","shell.execute_reply":"2024-11-23T11:51:34.824199Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preprocessing","metadata":{}},{"cell_type":"code","source":"def preprocess_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    # Remove punctuation and special characters\n    text = re.sub(r'[^a-z\\s]', '', text)\n    # Tokenize the text\n    tokens = text.split()\n    # Remove stop words\n    tokens = [word for word in tokens if word not in stop_words]\n    # Remove words not in the vocabulary\n    tokens = [word for word in tokens if word in vocabulary]\n    return tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:34.827693Z","iopub.execute_input":"2024-11-23T11:51:34.828439Z","iopub.status.idle":"2024-11-23T11:51:34.837796Z","shell.execute_reply.started":"2024-11-23T11:51:34.828388Z","shell.execute_reply":"2024-11-23T11:51:34.836849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply preprocessing to the 'email' column\nbalanced_df['tokens'] = balanced_df['email'].apply(preprocess_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:34.839071Z","iopub.execute_input":"2024-11-23T11:51:34.839400Z","iopub.status.idle":"2024-11-23T11:51:34.961636Z","shell.execute_reply.started":"2024-11-23T11:51:34.839368Z","shell.execute_reply":"2024-11-23T11:51:34.960594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# View the first few tokenized emails\nbalanced_df[['email', 'tokens']].head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:34.964421Z","iopub.execute_input":"2024-11-23T11:51:34.964796Z","iopub.status.idle":"2024-11-23T11:51:34.981047Z","shell.execute_reply.started":"2024-11-23T11:51:34.964760Z","shell.execute_reply":"2024-11-23T11:51:34.979934Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Vectorizing Emails Using Word2Vec Embeddings","metadata":{}},{"cell_type":"code","source":"embedding_size = 10  # As per your assignment\nmax_email_length = 12  # As specified in your assignment\n\n# def vectorize_email(email_tokens, word_embeddings):\n#     vectors = [word_embeddings[word] for word in email_tokens if word in word_embeddings]\n#     if vectors:\n#         return np.mean(vectors, axis=0)  # Shape: (embedding_size,)\n#     else:\n#         return np.zeros(embedding_size)\n\ndef vectorize_email(email_tokens, word_embeddings):\n    vectors = []\n    for word in email_tokens[:max_email_length]:\n        if word in word_embeddings:\n            vectors.append(word_embeddings[word])\n        else:\n            # Use a zero vector for unknown words (should be minimal due to preprocessing)\n            vectors.append(np.zeros(embedding_size))\n    # Pad with zero vectors if necessary\n    while len(vectors) < max_email_length:\n        vectors.append(np.zeros(embedding_size))\n    return np.array(vectors)  # Shape: (max_email_length, embedding_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:34.982545Z","iopub.execute_input":"2024-11-23T11:51:34.983050Z","iopub.status.idle":"2024-11-23T11:51:34.994136Z","shell.execute_reply.started":"2024-11-23T11:51:34.982973Z","shell.execute_reply":"2024-11-23T11:51:34.992873Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preparing Labels","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Assuming 'tokens' column contains the preprocessed tokens\nX = np.array([vectorize_email(tokens, word_embeddings) for tokens in balanced_df['tokens']])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:34.995500Z","iopub.execute_input":"2024-11-23T11:51:34.995875Z","iopub.status.idle":"2024-11-23T11:51:35.027131Z","shell.execute_reply.started":"2024-11-23T11:51:34.995839Z","shell.execute_reply":"2024-11-23T11:51:35.025897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y = balanced_df['label'].astype(int).values  # Shape: (num_samples,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:35.028725Z","iopub.execute_input":"2024-11-23T11:51:35.029205Z","iopub.status.idle":"2024-11-23T11:51:35.036055Z","shell.execute_reply.started":"2024-11-23T11:51:35.029153Z","shell.execute_reply":"2024-11-23T11:51:35.034656Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Neural Network","metadata":{}},{"cell_type":"markdown","source":"### Split data for training and test","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the data (e.g., 80% training, 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:35.040140Z","iopub.execute_input":"2024-11-23T11:51:35.046742Z","iopub.status.idle":"2024-11-23T11:51:35.054674Z","shell.execute_reply.started":"2024-11-23T11:51:35.046670Z","shell.execute_reply":"2024-11-23T11:51:35.053424Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Flatten the input samples\nX_train_flat = X_train.reshape(X_train.shape[0], -1)  # Shape: (num_train_samples, max_email_length * embedding_size)\nX_test_flat = X_test.reshape(X_test.shape[0], -1)     # Shape: (num_test_samples, max_email_length * embedding_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:35.056367Z","iopub.execute_input":"2024-11-23T11:51:35.056898Z","iopub.status.idle":"2024-11-23T11:51:35.066217Z","shell.execute_reply.started":"2024-11-23T11:51:35.056847Z","shell.execute_reply":"2024-11-23T11:51:35.064981Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initialization","metadata":{}},{"cell_type":"code","source":"# Number of input features\ninput_size = max_email_length * embedding_size  # 12 * 10 = 120\n\n# Initialize weights and biases\nnp.random.seed(42)  # For reproducibility\n\nhidden_layer_size = 8  # Increased from 8\n\n# Update weight and bias initializationsW1 = np.random.randn(input_size, 8) * np.sqrt(2 / input_size)\nW2 = np.random.randn(input_size, 8) * np.sqrt(2 / input_size)\n\n# W1 = np.random.randn(input_size, hidden_layer_size) * 0.01\nb1 = np.zeros((1, hidden_layer_size))\n\n# W2 = np.random.randn(input_size, hidden_layer_size) * 0.01\nb2 = np.zeros((1, hidden_layer_size))\n\nV = np.random.randn(hidden_layer_size * 2, 1) * 0.01\nc = np.zeros((1, 1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:35.067479Z","iopub.execute_input":"2024-11-23T11:51:35.067851Z","iopub.status.idle":"2024-11-23T11:51:35.080518Z","shell.execute_reply.started":"2024-11-23T11:51:35.067813Z","shell.execute_reply":"2024-11-23T11:51:35.079383Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Activiation Functions","metadata":{}},{"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    s = sigmoid(x)\n    return s * (1 - s)\n\ndef relu(x):\n    return np.maximum(0, x)\n\ndef relu_derivative(x):\n    return (x > 0).astype(float)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:35.081935Z","iopub.execute_input":"2024-11-23T11:51:35.082288Z","iopub.status.idle":"2024-11-23T11:51:35.099486Z","shell.execute_reply.started":"2024-11-23T11:51:35.082255Z","shell.execute_reply":"2024-11-23T11:51:35.098008Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Forward Propagation","metadata":{}},{"cell_type":"code","source":"def forward_propagation(X_sample):\n    # Hidden node 1\n    Z1 = np.dot(X_sample, W1) + b1\n    A1 = relu(Z1)\n    \n    # Hidden node 2\n    Z2 = np.dot(X_sample, W2) + b2\n    A2 = relu(Z2)\n    \n    # Concatenate activations\n    A_hidden = np.concatenate((A1, A2), axis=1)\n    \n    # Output node\n    Z_output = np.dot(A_hidden, V) + c\n    A_output = sigmoid(Z_output)\n    \n    cache = {\n        'X_sample': X_sample,\n        'Z1': Z1, 'A1': A1,\n        'Z2': Z2, 'A2': A2,\n        'A_hidden': A_hidden,\n        'Z_output': Z_output, 'A_output': A_output\n    }\n    return A_output, cache","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:35.101156Z","iopub.execute_input":"2024-11-23T11:51:35.101623Z","iopub.status.idle":"2024-11-23T11:51:35.115579Z","shell.execute_reply.started":"2024-11-23T11:51:35.101574Z","shell.execute_reply":"2024-11-23T11:51:35.114395Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Backward Propagation","metadata":{}},{"cell_type":"code","source":"def backward_propagation(y_true, cache):\n    # Retrieve cached values\n    X_sample = cache['X_sample']\n    Z1 = cache['Z1']\n    A1 = cache['A1']\n    Z2 = cache['Z2']\n    A2 = cache['A2']\n    A_hidden = cache['A_hidden']\n    Z_output = cache['Z_output']\n    A_output = cache['A_output']\n    \n    # Output layer gradients\n    dZ_output = A_output - y_true.reshape(-1, 1)\n    dV = np.dot(A_hidden.T, dZ_output)\n    dc = np.sum(dZ_output, axis=0, keepdims=True)\n    \n    dA_hidden = np.dot(dZ_output, V.T)\n    \n    # Split gradients\n    dA1 = dA_hidden[:, :hidden_layer_size]\n    dA2 = dA_hidden[:, hidden_layer_size:]\n    \n    dZ1 = dA1 * relu_derivative(Z1)\n    dZ2 = dA2 * relu_derivative(Z2)\n    \n    dW1 = np.dot(X_sample.T, dZ1)\n    db1 = np.sum(dZ1, axis=0, keepdims=True)\n    \n    dW2 = np.dot(X_sample.T, dZ2)\n    db2 = np.sum(dZ2, axis=0, keepdims=True)\n    \n    gradients = {\n        'dW1': dW1, 'db1': db1,\n        'dW2': dW2, 'db2': db2,\n        'dV': dV, 'dc': dc\n    }\n\n        # In backward_propagation function, after computing gradients\n    gradients['dW1'] += reg_strength * parameters['W1']\n    gradients['dW2'] += reg_strength * parameters['W2']\n    gradients['dV'] += reg_strength * parameters['V']\n    \n    return gradients","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:35.117186Z","iopub.execute_input":"2024-11-23T11:51:35.117638Z","iopub.status.idle":"2024-11-23T11:51:35.130893Z","shell.execute_reply.started":"2024-11-23T11:51:35.117588Z","shell.execute_reply":"2024-11-23T11:51:35.129663Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Update Parameters","metadata":{}},{"cell_type":"code","source":"def update_parameters(params, grads, learning_rate):\n    params['W1'] -= learning_rate * grads['dW1']\n    params['b1'] -= learning_rate * grads['db1']\n    params['W2'] -= learning_rate * grads['dW2']\n    params['b2'] -= learning_rate * grads['db2']\n    params['V'] -= learning_rate * grads['dV']\n    params['c'] -= learning_rate * grads['dc']\n    return params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:35.132487Z","iopub.execute_input":"2024-11-23T11:51:35.132858Z","iopub.status.idle":"2024-11-23T11:51:35.146883Z","shell.execute_reply.started":"2024-11-23T11:51:35.132826Z","shell.execute_reply":"2024-11-23T11:51:35.145713Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training Loop","metadata":{}},{"cell_type":"code","source":"# Pack parameters into a dictionary\nparameters = {\n    'W1': W1, 'b1': b1,\n    'W2': W2, 'b2': b2,\n    'V': V, 'c': c\n}\n\n# Training hyperparameters\nnum_epochs = 100\ninitial_lr = 0.01\ndecay = 0.001\n\nnum_samples = X_train_flat.shape[0]\n\nfor epoch in range(num_epochs):\n    total_loss = 0\n    for i in range(num_samples):\n        X_sample = X_train_flat[i].reshape(1, -1)  # Shape: (1, input_size)\n        y_sample = y_train[i]  # Scalar\n\n        # Forward propagation\n        A_output, cache = forward_propagation(X_sample)\n\n        # Compute loss (binary cross-entropy)\n        loss = - (y_sample * np.log(A_output + 1e-8) + (1 - y_sample) * np.log(1 - A_output + 1e-8))\n        # Inside the training loop, after computing the loss\n        reg_strength = 0.001  # Adjust as needed\n        loss = loss + (reg_strength / 2) * (\n            np.sum(np.square(parameters['W1'])) +\n            np.sum(np.square(parameters['W2'])) +\n            np.sum(np.square(parameters['V']))\n        )\n\n        total_loss += loss\n\n        # Backward propagation\n        gradients = backward_propagation(np.array([y_sample]), cache)\n\n        # Update parameters\n        learning_rate = initial_lr / (1 + decay * epoch)\n        parameters = update_parameters(parameters, gradients, learning_rate)\n\n    average_loss = total_loss / num_samples\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss[0][0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:35.148337Z","iopub.execute_input":"2024-11-23T11:51:35.148719Z","iopub.status.idle":"2024-11-23T11:51:46.922089Z","shell.execute_reply.started":"2024-11-23T11:51:35.148675Z","shell.execute_reply":"2024-11-23T11:51:46.920902Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluating Model","metadata":{}},{"cell_type":"code","source":"def predict(X, parameters):\n    predictions = []\n    for i in range(len(X)):\n        X_sample = X[i].reshape(1, -1)  # Shape: (1, input_size)\n        A_output, _ = forward_propagation(X_sample)\n        prediction = 1 if A_output >= 0.5 else 0\n        predictions.append(prediction)\n    return np.array(predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:46.923979Z","iopub.execute_input":"2024-11-23T11:51:46.924444Z","iopub.status.idle":"2024-11-23T11:51:46.931177Z","shell.execute_reply.started":"2024-11-23T11:51:46.924392Z","shell.execute_reply":"2024-11-23T11:51:46.929895Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\n# Predictions on test set\npredictions_test = predict(X_test_flat, parameters)\n\n# Calculate metrics\naccuracy = accuracy_score(y_test, predictions_test)\nprecision = precision_score(y_test, predictions_test)\nrecall = recall_score(y_test, predictions_test)\nf1 = f1_score(y_test, predictions_test)\n\n# Print the metrics\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1 Score: {f1:.2f}\")\n\n# Compute confusion matrix\ncm = confusion_matrix(y_test, predictions_test)\n\n# Print confusion matrix\nprint(\"Confusion Matrix:\")\nprint(cm)\n\n# Visualize confusion matrix\nfig, ax = plt.subplots()\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Not Spam', 'Spam'])\ndisp.plot(ax=ax)\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:46.932666Z","iopub.execute_input":"2024-11-23T11:51:46.933128Z","iopub.status.idle":"2024-11-23T11:51:47.230766Z","shell.execute_reply.started":"2024-11-23T11:51:46.933079Z","shell.execute_reply":"2024-11-23T11:51:47.229553Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Classification Threshold","metadata":{}},{"cell_type":"code","source":"def predict_probabilities(X, parameters):\n    probabilities = []\n    for i in range(len(X)):\n        X_sample = X[i].reshape(1, -1)  # Shape: (1, input_size)\n        A_output, _ = forward_propagation(X_sample)\n        probabilities.append(A_output[0][0])\n    return np.array(probabilities)  # Shape: (num_samples,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:47.232065Z","iopub.execute_input":"2024-11-23T11:51:47.232428Z","iopub.status.idle":"2024-11-23T11:51:47.238794Z","shell.execute_reply.started":"2024-11-23T11:51:47.232375Z","shell.execute_reply":"2024-11-23T11:51:47.237377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\nfpr, tpr, roc_thresholds = roc_curve(y_test, y_scores)\nroc_auc = auc(fpr, tpr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:47.240318Z","iopub.execute_input":"2024-11-23T11:51:47.240659Z","iopub.status.idle":"2024-11-23T11:51:47.251642Z","shell.execute_reply.started":"2024-11-23T11:51:47.240623Z","shell.execute_reply":"2024-11-23T11:51:47.250480Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Get predicted probabilities\ny_scores = predict_probabilities(X_test_flat, parameters)\n\n# Step 2: Define thresholds\nthresholds = np.linspace(0, 1, 101)\n\n# Step 3: Compute precision, recall, and F1 score for each threshold\nprecision_scores = []\nrecall_scores = []\nf1_scores = []\n\nfor threshold in thresholds:\n    predictions = (y_scores >= threshold).astype(int)\n    precision = precision_score(y_test, predictions, zero_division=0)\n    recall = recall_score(y_test, predictions, zero_division=0)\n    f1 = f1_score(y_test, predictions, zero_division=0)\n    precision_scores.append(precision)\n    recall_scores.append(recall)\n    f1_scores.append(f1)\n\n# Step 4: Plot Precision-Recall-F1 vs. Threshold\nplt.figure(figsize=(8, 6))\nplt.plot(thresholds, precision_scores, label='Precision', color='blue')\nplt.plot(thresholds, recall_scores, label='Recall', color='green')\nplt.plot(thresholds, f1_scores, label='F1 Score', color='red')\nplt.xlabel('Threshold')\nplt.ylabel('Score')\nplt.title('Precision, Recall, and F1 Score vs. Threshold')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Step 5: Find the optimal threshold based on maximum F1 score\nbest_f1_index = np.argmax(f1_scores)\noptimal_threshold = thresholds[best_f1_index]\nprint(f'Optimal Threshold based on F1 Score: {optimal_threshold:.2f}')\n\n# Step 6: Make predictions using the optimal threshold\npredictions_adjusted = (y_scores >= optimal_threshold).astype(int)\n\n# Step 7: Evaluate the model with the adjusted threshold\naccuracy_adjusted = accuracy_score(y_test, predictions_adjusted)\nprecision_adjusted = precision_score(y_test, predictions_adjusted)\nrecall_adjusted = recall_score(y_test, predictions_adjusted)\nf1_adjusted = f1_score(y_test, predictions_adjusted)\ncm_adjusted = confusion_matrix(y_test, predictions_adjusted)\n\nprint(f\"Adjusted Test Accuracy: {accuracy_adjusted * 100:.2f}%\")\nprint(f\"Adjusted Precision: {precision_adjusted:.2f}\")\nprint(f\"Adjusted Recall: {recall_adjusted:.2f}\")\nprint(f\"Adjusted F1 Score: {f1_adjusted:.2f}\")\nprint(\"Adjusted Confusion Matrix:\")\nprint(cm_adjusted)\n\n# Step 8: Plot the ROC Curve\nfpr, tpr, roc_thresholds = roc_curve(y_test, y_scores)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate (Recall)')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T11:51:47.255150Z","iopub.execute_input":"2024-11-23T11:51:47.255534Z","iopub.status.idle":"2024-11-23T11:51:48.119628Z","shell.execute_reply.started":"2024-11-23T11:51:47.255497Z","shell.execute_reply":"2024-11-23T11:51:48.118348Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}